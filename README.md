# ğŸ“˜ DocQueryEngine

A document ingestion and Retrieval-Augmented Generation (RAG)-based Q&A backend built with FastAPI, PostgreSQL + pgvector, Hugging Face Transformers, Langchain, and spaCy.

---

## ğŸš€ Features

- âœ… Upload `.pdf` and `.txt` files
- âœ… Extract and embed document content
- âœ… Ask questions against specific documents
- âœ… JWT-based authentication
- âœ… Postgres + pgvector storage
- âœ… FAISS retrieval support
- âœ… Dockerized with Gunicorn for production
- âœ… Environment-based config support

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/                  # FastAPI routes
â”‚   â”œâ”€â”€ core/                 # Embedding, retrieval, Q&A logic
â”‚   â”œâ”€â”€ exception/            # Exception handler
â”‚   â”œâ”€â”€ middleware/           # Middlewares for the application
â”‚   â”œâ”€â”€ models/               # SQLAlchemy models
â”‚   â”œâ”€â”€ service/              # Service layer 
â”‚   â”œâ”€â”€ util/                 # Utlities for error, logging etc.
â”œâ”€â”€ tests/                    # Containe unit tests for some endpoints and services
â”‚   â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ service/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.dev
â”œâ”€â”€ .env.prod
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ³ Running with Docker

### Run PostgreSQL with pgvector using Docker Compose

Create a `docker-compose.yml` file like this:

```yaml
version: "3.8"

services:
  postgres:
    image: ankane/pgvector:latest
    container_name: rag_pgvector
    restart: always
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: ragdb
    volumes:
      - pgdata:/var/lib/postgresql/data

volumes:
  pgdata:
```

Then run:

```bash
docker-compose up -d
```

This will start the PostgreSQL database with pgvector extension enabled, ready to accept connections on `localhost:5432`.

---

### Build and run FastAPI container

```bash
docker build -t doc-query-engine .
docker run --rm -p 8000:8000 --network rag-network doc-query-engine
```

---

## âš™ï¸ Environment Variables

Environment-specific files:

- `.env.dev` â†’ used for local development
- `.env.prod` â†’ used inside Docker container

Example:

```env
DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost:5432/ragdb
SECRET_KEY=supersecret
EMBEDDING_DIM=1024
```

---

## ğŸ§ª Run Locally with Uvicorn

```bash
uvicorn main:app --reload
```

Ensure your `.env.dev` is active and database is running locally or via Docker.

---

## ğŸ” Auth Endpoints

- `POST /api/v1/register`
- `POST /api/v1/login`

Uses access and refresh JWTs. Pass `Authorization: Bearer <access_token>` in secure endpoints.

---

## ğŸ§ Document Endpoints

- `POST /api/v1/ingest` â†’ Ingest document (auth required)
- `POST /api/v1//list-documents` â†’ Listing all uploaded documents by user
- `POST /api/v1/qa` â†’ Ask question using `document_id`

---

## ğŸš€ Deployment to AWS ECR

### 1. Authenticate Docker with AWS

```bash
aws ecr get-login-password --region <your-region> | docker login --username AWS --password-stdin <your-account-id>.dkr.ecr.<your-region>.amazonaws.com
```

---

### 2. Create ECR Repository (if not created)

```bash
aws ecr create-repository --repository-name doc-query-engine --region <your-region>
```

---

### 3. Build and Tag Docker Image

```bash
docker build -t doc-query-engine .
docker tag doc-query-engine:latest <your-account-id>.dkr.ecr.<your-region>.amazonaws.com/doc-query-engine:latest
```

---

### 4. Push to ECR

```bash
docker push <your-account-id>.dkr.ecr.<your-region>.amazonaws.com/doc-query-engine:latest
```

---

### 5. Deploy via ECS, Fargate, or EC2

You can now pull and deploy the container using:

- **ECS with Fargate**
- **EC2** (`docker run`)
- **Elastic Beanstalk** (multi-container)
- **EKS (Kubernetes)** with custom Helm chart

---

## ğŸ“‰ GitHub CI/CD Pipeline Setup

### 1. Create `.github/workflows/ci.yml`

```yaml
name: CI Pipeline

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run tests
        run: |
          pytest

  docker:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v3

      - name: Log in to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v1

      - name: Build and push Docker image
        env:
          ECR_REGISTRY: <your-account-id>.dkr.ecr.<your-region>.amazonaws.com
          ECR_REPOSITORY: doc-query-engine
        run: |
          docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:latest .
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest
```

### 2. Store secrets in GitHub repo settings
- `AWS_ACCESS_KEY_ID`
- `AWS_SECRET_ACCESS_KEY`
- `AWS_REGION`

---

## ğŸ›  Design Patterns Used

- **Service Layer** â€“ business logic in `service/`
- **Dependency Injection** â€“ via `Depends()`
- **Repository Pattern** â€“ DB queries abstracted via SQLAlchemy
- **Strategy Pattern** â€“ supports FAISS and pgvector as interchangeable retrievers

---

## Enhancements

- Adding validations for limiting document upload size
- Ability to accept multiple documents for ingestion
- Ability to ingest multiple documents with Background Tasks 
- Maintaining a chat-history upto last k-number of chats
- Use of Redis with Celery for efficient cache management
- Role based authorization for users
- - Maintaining user-profiles for verification and activation.
- Maintaining token limit for users for a certain amount of time
- Maintain and training separate ML models
- Adding support for other documents like .docx, .json etc.
 

---

## âœ… License

MIT License
